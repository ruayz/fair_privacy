{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yx23/anaconda3/envs/privacy_meter/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from audit import audit_models, audit_records, sample_auditing_dataset, exp_estimated_epsilon\n",
    "from get_signals import get_model_signals\n",
    "from models.utils import load_models, train_models, split_dataset_for_training\n",
    "from util import (\n",
    "    check_configs,\n",
    "    setup_log,\n",
    "    initialize_seeds,\n",
    "    create_directories,\n",
    "    load_dataset,\n",
    "    load_subset_dataset,\n",
    ")\n",
    "\n",
    "# Enable benchmark mode in cudnn to improve performance when input sizes are consistent\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = \"configs/tabular/adult/adult_dpsgdfga.yaml\"\n",
    "with open(configs, \"rb\") as f:\n",
    "    configs = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "# Validate configurations\n",
    "check_configs(configs)\n",
    "\n",
    "# Initialize seeds for reproducibility\n",
    "initialize_seeds(configs[\"run\"][\"random_seed\"])\n",
    "\n",
    "# Create necessary directories\n",
    "subdata_dir = configs[\"train\"][\"log_dir\"] \n",
    "log_dir = subdata_dir + configs[\"train\"][\"model_name\"] + \"/\" + configs[\"train\"][\"method\"]\n",
    "if configs[\"train\"][\"epsilon\"] > 0:\n",
    "    log_dir += f\"/eps{int(configs[\"train\"][\"epsilon\"])}\"\n",
    "configs[\"train\"][\"log_dir\"] = log_dir\n",
    "configs[\"run\"][\"log_dir\"] = log_dir\n",
    "\n",
    "directories = {\n",
    "    \"log_dir\": log_dir,\n",
    "    \"report_dir\": f\"{log_dir}/report\",\n",
    "    #\"signal_dir\": f\"{log_dir}/signals\",\n",
    "    \"data_dir\": configs[\"data\"][\"data_dir\"],\n",
    "    \"subdata_dir\": subdata_dir,\n",
    "}\n",
    "create_directories(directories)\n",
    "\n",
    "# Set up logger\n",
    "logger = setup_log(\n",
    "    directories[\"report_dir\"], \"time_analysis\", configs[\"run\"][\"time_log\"]\n",
    ")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/demo_tabular/adult/LR/dpsgdfga/eps10'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 20:29:33,100 INFO     Load data from data/tabular/adult.pkl\n",
      "12/23/2024 20:29:33:INFO:Load data from data/tabular/adult.pkl\n",
      "2024-12-23 20:29:33,102 INFO     The whole dataset size: 45222\n",
      "12/23/2024 20:29:33:INFO:The whole dataset size: 45222\n",
      "2024-12-23 20:29:33,103 INFO     Loading dataset took 0.02563 seconds\n",
      "12/23/2024 20:29:33:INFO:Loading dataset took 0.02563 seconds\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "dataset = load_dataset(configs, directories[\"data_dir\"], logger)\n",
    "logger.info(\"Loading dataset took %0.5f seconds\", time.time() - baseline_time)\n",
    "\n",
    "# # subset of dataset\n",
    "# if configs[\"train\"][\"data_size\"] < len(dataset):\n",
    "#     dataset = load_subset_dataset(configs, dataset, f\"{directories[\"subdata_dir\"]}\", logger)\n",
    "#     logger.info(\"Loading sub-dataset took %0.5f seconds\", time.time() - baseline_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 21:03:10,327 INFO     Loading model 0\n",
      "12/17/2024 21:03:10:INFO:Loading model 0\n"
     ]
    }
   ],
   "source": [
    "# Define experiment parameters\n",
    "model_num = 1\n",
    "num_model_pairs = 1\n",
    "\n",
    "# Load or train models\n",
    "baseline_time = time.time()\n",
    "models_list, memberships = load_models(\n",
    "    log_dir, dataset, model_num, configs, logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_list is None:\n",
    "    data_splits, memberships = split_dataset_for_training(\n",
    "        len(dataset), num_model_pairs, ratio=0.5\n",
    "    )\n",
    "    data_splits.pop()\n",
    "    memberships = memberships[:-1]\n",
    "    models_list = train_models(\n",
    "        log_dir, dataset, data_splits, memberships[:-1], configs, logger\n",
    "    )\n",
    "logger.info(\n",
    "    \"Model loading/training took %0.1f seconds\", time.time() - baseline_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare auditing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships = np.array([memberships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auditing_dataset, auditing_membership = sample_auditing_dataset(\n",
    "#         configs, dataset, logger, memberships\n",
    "#     )\n",
    "auditing_dataset, auditing_membership = dataset, memberships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing softmax: 100%|██████████| 3/3 [00:00<00:00, 19.04it/s]\n",
      "2024-12-17 21:03:49,067 INFO     Signals saved to disk.\n",
      "12/17/2024 21:03:49:INFO:Signals saved to disk.\n",
      "2024-12-17 21:03:49,071 INFO     Preparing signals took 2.16958 seconds\n",
      "12/17/2024 21:03:49:INFO:Preparing signals took 2.16958 seconds\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "configs[\"audit\"][\"algorithm\"] == \"LOSS\"\n",
    "signals = get_model_signals(models_list, auditing_dataset, configs, logger)\n",
    "logger.info(\"Preparing signals took %0.5f seconds\", time.time() - baseline_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 21:04:41,045 INFO     mia_score_list and membership_list saved to disk.\n",
      "12/17/2024 21:04:41:INFO:mia_score_list and membership_list saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Perform the privacy audit\n",
    "baseline_time = time.time()\n",
    "#target_model_indices = list(range(num_experiments))\n",
    "target_model_indices = list(range(model_num)) # for all models\n",
    "\n",
    "# shape of mia_score_list: (n, m) n=(num_reference_models+1)*2, m=len(auditing_dataset)\n",
    "mia_score_list, membership_list = audit_records(\n",
    "    f\"{directories['report_dir']}\",\n",
    "    target_model_indices,\n",
    "    signals,\n",
    "    auditing_membership,\n",
    "    model_num,\n",
    "    logger,\n",
    "    attack_algorithm='LOSS',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.13504328, -0.07227418, -0.06299163, ..., -0.05772954,\n",
       "        -0.10837515, -0.1222467 ], dtype=float32),\n",
       " array([0, 0, 1, ..., 1, 1, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mia_score_list.flatten(), membership_list.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5033595360718892\n",
      "0.5090255215289552\n",
      "0.5043626448534424\n",
      "0.49363147792706336\n",
      "0.5010946262272444\n",
      "0.4755399504307801\n",
      "0.5020277667197452\n",
      "0.5107132331270262\n",
      "0.47030107490649325\n",
      "0.5110708928454967\n"
     ]
    }
   ],
   "source": [
    "#算auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "all_auc = {i:[] for i in range(10)}\n",
    "targets = []\n",
    "for i,j in dataset:\n",
    "    targets.append(j)\n",
    "targets = np.array(targets)\n",
    "\n",
    "s = mia_score_list.flatten()\n",
    "m = membership_list.flatten().astype(int)\n",
    "fpr_list, tpr_list, _ = roc_curve(m, s)\n",
    "for i in range(10):\n",
    "    fpr_list, tpr_list, _ = roc_curve(m[targets==i], s[targets==i])\n",
    "    roc_auc = auc(fpr_list, tpr_list)   \n",
    "    all_auc[i].append(roc_auc)\n",
    "\n",
    "#每一类的平均auc\n",
    "for i in range(10):\n",
    "    print(np.mean(all_auc[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacy_meter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
