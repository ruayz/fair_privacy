{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yx23/anaconda3/envs/privacy_meter/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "from audit import get_average_audit_results, audit_models, audit_records, sample_auditing_dataset, exp_estimated_epsilon\n",
    "from get_signals import get_model_signals\n",
    "from models.utils import load_models, train_models, split_dataset_for_training\n",
    "from util import (\n",
    "    check_configs,\n",
    "    setup_log,\n",
    "    initialize_seeds,\n",
    "    create_directories,\n",
    "    load_dataset,\n",
    "    load_subset_dataset,\n",
    ")\n",
    "\n",
    "# Enable benchmark mode in cudnn to improve performance when input sizes are consistent\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = \"configs/mnist.yaml\"\n",
    "with open(configs, \"rb\") as f:\n",
    "    configs = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "# Validate configurations\n",
    "check_configs(configs)\n",
    "\n",
    "# Initialize seeds for reproducibility\n",
    "initialize_seeds(configs[\"run\"][\"random_seed\"])\n",
    "\n",
    "# Create necessary directories\n",
    "subdata_dir = configs[\"run\"][\"log_dir\"] \n",
    "log_dir = configs[\"run\"][\"log_dir\"] + configs[\"train\"][\"model_name\"] #+ \"/\" + configs[\"train\"][\"method\"]\n",
    "if configs[\"train\"][\"epsilon\"] > 0:\n",
    "    log_dir += f\"/eps{int(configs[\"train\"][\"epsilon\"])}\"\n",
    "else:\n",
    "    log_dir += \"/nonpri\"\n",
    "configs[\"run\"][\"log_dir\"] = log_dir\n",
    "directories = {\n",
    "    \"log_dir\": log_dir,\n",
    "    \"report_dir\": f\"{log_dir}/report\",\n",
    "    \"signal_dir\": f\"{log_dir}/signals\",\n",
    "    \"data_dir\": configs[\"data\"][\"data_dir\"],\n",
    "    \"subdata_dir\": subdata_dir,\n",
    "}\n",
    "create_directories(directories)\n",
    "\n",
    "# Set up logger\n",
    "logger = setup_log(\n",
    "    directories[\"report_dir\"], \"time_analysis\", configs[\"run\"][\"time_log\"]\n",
    ")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp/demo_mnist/LR/eps100'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 11:20:35,738 INFO     Load data from data/mnist.pkl\n",
      "12/18/2024 11:20:35:INFO:Load data from data/mnist.pkl\n",
      "2024-12-18 11:20:35,740 INFO     The whole dataset size: 70000\n",
      "12/18/2024 11:20:35:INFO:The whole dataset size: 70000\n",
      "2024-12-18 11:20:35,741 INFO     Loading dataset took 0.03804 seconds\n",
      "12/18/2024 11:20:35:INFO:Loading dataset took 0.03804 seconds\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "dataset = load_dataset(configs, directories[\"data_dir\"], logger)\n",
    "logger.info(\"Loading dataset took %0.5f seconds\", time.time() - baseline_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 11:20:38,798 INFO     Load data from exp/demo_mnist/mnist.pkl\n",
      "12/18/2024 11:20:38:INFO:Load data from exp/demo_mnist/mnist.pkl\n",
      "2024-12-18 11:20:38,803 INFO     Loading sub-dataset took 3.10018 seconds\n",
      "12/18/2024 11:20:38:INFO:Loading sub-dataset took 3.10018 seconds\n"
     ]
    }
   ],
   "source": [
    "# subset of dataset\n",
    "if configs[\"train\"][\"data_size\"] < len(dataset):\n",
    "    dataset = load_subset_dataset(configs, dataset, f\"{directories[\"subdata_dir\"]}\", logger)\n",
    "    logger.info(\"Loading sub-dataset took %0.5f seconds\", time.time() - baseline_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment parameters\n",
    "num_experiments = configs[\"run\"][\"num_experiments\"]\n",
    "num_reference_models = configs[\"audit\"][\"num_ref_models\"]\n",
    "num_model_pairs = max(math.ceil(num_experiments / 2.0), num_reference_models + 1)\n",
    "\n",
    "# Load or train models\n",
    "baseline_time = time.time()\n",
    "models_list, memberships = load_models(\n",
    "    log_dir, dataset, num_model_pairs * 2, configs, logger\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 11:11:36,050 INFO     Model loading/training took 2.9 seconds\n",
      "12/18/2024 11:11:36:INFO:Model loading/training took 2.9 seconds\n"
     ]
    }
   ],
   "source": [
    "if models_list is None:\n",
    "    # Split dataset for training two models per pair\n",
    "    data_splits, memberships = split_dataset_for_training(\n",
    "        len(dataset), num_model_pairs, ratio=0.5\n",
    "    )\n",
    "    models_list = train_models(\n",
    "        log_dir, dataset, data_splits, memberships, configs, logger\n",
    "    )\n",
    "logger.info(\n",
    "    \"Model loading/training took %0.1f seconds\", time.time() - baseline_time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare auditing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auditing_dataset, auditing_membership = sample_auditing_dataset(\n",
    "#         configs, dataset, logger, memberships\n",
    "#     )\n",
    "auditing_dataset, auditing_membership = dataset, memberships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 11:20:46,190 INFO     Signals loaded from disk.\n",
      "12/18/2024 11:20:46:INFO:Signals loaded from disk.\n",
      "2024-12-18 11:20:46,191 INFO     Preparing signals took 0.00582 seconds\n",
      "12/18/2024 11:20:46:INFO:Preparing signals took 0.00582 seconds\n"
     ]
    }
   ],
   "source": [
    "baseline_time = time.time()\n",
    "signals = get_model_signals(models_list, auditing_dataset, configs, logger)\n",
    "logger.info(\"Preparing signals took %0.5f seconds\", time.time() - baseline_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 11:20:48,180 INFO     mia_score_list and membership_list loaded to disk.\n",
      "12/18/2024 11:20:48:INFO:mia_score_list and membership_list loaded to disk.\n"
     ]
    }
   ],
   "source": [
    "# Perform the privacy audit\n",
    "baseline_time = time.time()\n",
    "#target_model_indices = list(range(num_experiments))\n",
    "target_model_indices = list(range((num_reference_models+1)*2)) # for all pair models\n",
    "\n",
    "# shape of mia_score_list: (n, m) n=(num_reference_models+1)*2, m=len(auditing_dataset)\n",
    "mia_score_list, membership_list = audit_records(\n",
    "    f\"{directories['report_dir']}\",\n",
    "    target_model_indices,\n",
    "    signals,\n",
    "    auditing_membership,\n",
    "    num_reference_models,\n",
    "    logger,\n",
    "    attack_algorithm=configs[\"audit\"][\"algorithm\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(target_model_indices) > 1:\n",
    "#     logger.info(\n",
    "#         \"Auditing privacy risk took %0.1f seconds\", time.time() - baseline_time\n",
    "#     )\n",
    "\n",
    "# # Get average audit results across all experiments\n",
    "# if len(target_model_indices) > 1:\n",
    "#     get_average_audit_results(\n",
    "#         directories[\"report_dir\"], mia_score_list, membership_list, logger\n",
    "#     )\n",
    "\n",
    "#logger.info(\"Total runtime: %0.5f seconds\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = directories['report_dir'] + \"/pro_attributes.npy\"\n",
    "# pro_attributes = []\n",
    "# for idx, (_, _, z) in enumerate(dataset):\n",
    "#     pro_attributes.append((idx, z))\n",
    "# pro_attributes = np.array(pro_attributes)\n",
    "# np.save(save_path, pro_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit by epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eps = exp_estimated_epsilon(\n",
    "    f\"{directories['report_dir']}/{configs[\"audit\"][\"algorithm\"].lower()}\", \n",
    "    mia_score_list, \n",
    "    membership_list, \n",
    "    dataset, \n",
    "    configs, \n",
    "    logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5646255319148936\n",
      "0.5432588809946715\n",
      "0.601818968968969\n",
      "0.6027196376101861\n",
      "0.5981981025641026\n",
      "0.6314512195121951\n",
      "0.5766478636826042\n",
      "0.5787186660268714\n",
      "0.6233127692307693\n",
      "0.6105920020120723\n"
     ]
    }
   ],
   "source": [
    "#算auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "all_auc = {i:[] for i in range(10)}\n",
    "for i, (s, m) in enumerate(zip(mia_score_list, membership_list)):\n",
    "    m = m.astype(int)\n",
    "    fpr_list, tpr_list, _ = roc_curve(m, s)\n",
    "    roc_auc = auc(fpr_list, tpr_list)\n",
    "    all_auc[dataset[i][1]].append(roc_auc)\n",
    "    # _, emp_eps_loss = compute_eps_lower_from_mia(s, m, 0.05, configs[\"train\"][\"delta\"], 'GDP', n_procs=32)\n",
    "    # all_eps[dataset[i][1]].append(emp_eps_loss)\n",
    "\n",
    "#每一类的平均auc\n",
    "for i in range(10):\n",
    "    print(np.mean(all_auc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.42922727343179023\n",
      "1:0.28738560845276107\n",
      "2:0.6747254973767883\n",
      "3:0.6408867989409675\n",
      "4:0.5466770951671491\n",
      "5:0.7702012655963631\n",
      "6:0.47327895370018097\n",
      "7:0.475372824827916\n",
      "8:0.7973298190483309\n",
      "9:0.6153794759383451\n"
     ]
    }
   ],
   "source": [
    "#每一类的平均\n",
    "for i in range(10):\n",
    "    print(f\"{i}:{np.mean(all_eps[i])}, {np.std(all_eps[i])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacy_meter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
